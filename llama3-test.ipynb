{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Pytorch evaluates env vars on import instead of runtime so this needs to be at the top\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = './Meta-Llama-3-8B-Instruct/'\n",
    "tokenizer_path = 'Meta-Llama-3-8B-Instruct/tokenizer.model'\n",
    "ckpt_path = 'Meta-Llama-3-8B-Instruct/consolidated.00.pth'\n",
    "params_path = 'Meta-Llama-3-8B-Instruct/params.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanewinterhalter/Projects/security_tool/_venv/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 40.14 seconds\n"
     ]
    }
   ],
   "source": [
    "llama = Llama.build(ckpt_dir, tokenizer_path, max_seq_len=512, model_parallel_size=1, max_batch_size=4, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanewinterhalter/Projects/security_tool/llama3/llama/generation.py:218: UserWarning: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  torch.isin(next_token, stop_tokens)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generation': {'role': 'assistant', 'content': 'The'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First generation to preload model\n",
    "llama.chat_completion(\n",
    "    [[{\"role\": \"user\", \"content\": \"abcde\"}]],\n",
    "    max_gen_len=1,\n",
    "    temperature=0,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.34 s ± 13.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit results = llama.chat_completion([messages], max_gen_len=10, temperature=0.7, top_p=0.9)\n",
    "# 2.39s without torch compile & MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
